import os
import json
import openai
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
from kbs_crolling import search_kbs_news
from mbc_crolling import search_mbc_news
from sbs_crolling import search_sbs_news
from crolling import data_crawl
from job_crawl import crawl_jobkorea

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ì½ì–´ì˜´
api_key = os.getenv("OPENAI_API_KEY")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = OpenAI(api_key=api_key)

# Streamlit ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
st.title("JOB.PT")
st.sidebar.header("ì§ì—… ì¶”ì²œ ë° í•„ìš” ì—­ëŸ‰ ë¶„ì„")
q = st.sidebar.text_input("ë¶„ì•¼ë¥¼ ì…ë ¥í•˜ì„¸ìš”", "")

if not q:
    st.warning("ë¶„ì•¼ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")  # ì…ë ¥ ìœ ë„ê°€ ì—†ì„ ê²½ìš° ë©”ì‹œì§€ ì¶œë ¥
else:
    try:
        # ë…¼ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°
        data_crawl(q) 

        # 3ëŒ€ ë‰´ìŠ¤ í¬ë¡¤ë§
        search_kbs_news(q, max_results=5)  # KBS ë‰´ìŠ¤ í¬ë¡¤ë§
        search_mbc_news(q, max_news=5)  # MBC ë‰´ìŠ¤ í¬ë¡¤ë§
        search_sbs_news(q, total_news=5)  # SBS ë‰´ìŠ¤ í¬ë¡¤ë§
        
        with open("data.txt", "r", encoding="utf-8") as file:
            trend = file.read()
        with open("news.txt", "r", encoding="utf-8") as file:
            news = file.read()


    except FileNotFoundError:
        st.error("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'data.txt'ì™€ 'news.txt'ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    answers = []

    gif_image_url = "https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExY214cXViejhkajA4bXhhc2RkdGIxeTV2YmtwZmphZ2VqMXR4bnUwMCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/lJNoBCvQYp7nq/giphy.webp"  # ê¸°ë³¸ ì´ë¯¸ì§€ URL

    # ì§„í–‰ë¥  í‘œì‹œìš© ë°”
    progress_bar = st.progress(0)  # ì´ˆê¸° ì§„í–‰ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì •

    # ì‹¤í–‰ ì¤‘ ìƒíƒœ í‘œì‹œ
    text_placeholder = st.empty()  # í…ìŠ¤íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ì œì–´í•˜ê¸° ìœ„í•œ placeholder
    loading_placeholder = st.empty()
    text_placeholder.write("**Self-Consistency ì‹¤í–‰ ì¤‘... ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!**")
    loading_placeholder.markdown(
        f'<img src="{gif_image_url}" alt="loading gif">',
        unsafe_allow_html=True,
    )

    # Self-Consistency ìˆ˜í–‰
    for i in range(3):
        # ì²« ë²ˆì§¸ ìš”ì²­ / ê¸°ì‚¬ ê¸°ë°˜
        response1 = client.chat.completions.create(
            model="gpt-4o-mini",
            response_format={ "type": "json_object" },
            messages=[
                {"role": "system", "content": "You are a helpful assistant designed to output JSON. Please write in Korean."},
                {"role": "user", "content": f"ì‚¬íšŒ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {news}. ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ '{q}'ì™€ ì—°ê´€ëœ ì‚¬íšŒ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ê³ , ì¤‘ê°„ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ì‹œì˜¤."}
            ]
        )
        result1 = response1.choices[0].message.content

        # ë‘ ë²ˆì§¸ ìš”ì²­ / ë…¼ë¬¸ ê¸°ë°˜
        response2 = client.chat.completions.create(
            model="gpt-4o-mini",
            response_format={ "type": "json_object" },
            messages=[
                {"role": "system", "content": "You are a helpful assistant designed to output JSON. Please write in Korean."},
                {"role": "user", "content": f"ì´ì „ ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {result1}. ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ '{trend}'ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¶”ì²œí•  ë§Œí•œ ê´€ë ¨ ì§ì—… 3ê°œë¥¼ ëª…ì‹œí•˜ì„¸ìš”. ì¶œë ¥ê°’ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”. ë‹¨ ì§ì—…ì„ êµ¬ì²´ì ì¸ ì§ì—…ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”. {{'ì§ì—…':[ì˜ì‚¬,íšŒê³„ì‚¬,ì‘ê³¡ê°€]}}."}
            ]
        )
        result2 = response2.choices[0].message.content
        result2 = json.loads(result2)
        answers.append(result2["ì§ì—…"])


        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        progress_bar.progress(int((i + 1) * 33)+1)  

    # ìµœì¢… ìš”ì²­
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        response_format={ "type": "json_object" },
        messages=[
            {"role": "system", "content": "You are a helpful assistant designed to output JSON. Please write in Korean."},
            {"role": "user", "content": f"{answers}ì— ë‚˜ì˜¨ 9ê°œì˜ ì§ì—…ì¤‘ì—ì„œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ì§ì—… 3ê°œë¥¼ ë½‘ì•„ì£¼ê³ , ê° ì§ì—…ì— ëŒ€í•œ í•„ìš” ì—­ëŸ‰ì„ ë‹µë³€í•˜ì„¸ìš”. ì¶œë ¥ê°’ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”. {{'ì§ì—…':[ì˜ì‚¬,íšŒê³„ì‚¬,ì‘ê³¡ê°€], 'í•„ìš”ì—­ëŸ‰':[ì˜ì‚¬ìê²©ì¦,ìˆ˜í•™ì§€ì‹,ìŒì•…ê°ê°]}}. ì§ì—…[0]ì— ëŒ€í•œ í•„ìš”ì—­ëŸ‰ì€ í•„ìš”ì—­ëŸ‰[0]ì— í•´ë‹¹í•˜ëŠ” ë°©ì‹ì´ë‹¤."}
        ]   
    )

    # ê²°ê³¼ ì¶œë ¥
    result = json.loads(response.choices[0].message.content)

    # ë¡œë”© ìƒíƒœ ì œê±°
    loading_placeholder.empty()  # GIF ì œê±°
    text_placeholder.empty()  # í…ìŠ¤íŠ¸ ì œê±°
    progress_bar.empty()  # ì§„í–‰ë¥  ë°” ì œê±°
    st.write("**Self-Consistency ì‹¤í–‰ ì™„ë£Œ!** âœ…")

    # ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ ì˜ˆì˜ê²Œ ì¶œë ¥
    st.subheader("ğŸ” ë¶„ì„ ê²°ê³¼")
    st.write("### ì¶”ì²œ ì§ì—…")
    for job, skill in zip(result["ì§ì—…"], result["í•„ìš”ì—­ëŸ‰"]):
        st.markdown(f"- **ì§ì—…**: {job}  <br>  **í•„ìš” ì—­ëŸ‰**: {skill}", unsafe_allow_html=True)
        job_list = crawl_jobkorea(job)

    st.subheader("ğŸ”— ì¶”ì²œ ì§ì—… ë° í•„ìš” ì—­ëŸ‰ ë¶„ì„")

    for job, skill in zip(result["ì§ì—…"], result["í•„ìš”ì—­ëŸ‰"]):
        st.markdown(f"#### **ì§ì—…**: {job}")
        st.markdown(f"- **í•„ìš” ì—­ëŸ‰**: {skill}")
        
        # JobKorea í¬ë¡¤ë§ ë°ì´í„° í‘œì‹œ
        st.write("**ê´€ë ¨ ê³µê³ :**")
        job_list = crawl_jobkorea(job)
        if job_list:
            for idx, job_info in enumerate(job_list, 1):
                st.write(f"{idx}. [{job_info['title']}](<{job_info['link']}>)")
        else:
            st.write("ê´€ë ¨ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

